{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo a primeira página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apresentação opt_01\n",
      "Produção opt_02\n",
      "Processamento opt_03\n",
      "Comercialização opt_04\n",
      "Importação opt_05\n",
      "Exportação opt_06\n",
      "Publicação opt_07\n"
     ]
    }
   ],
   "source": [
    "embrapa_url = \"http://vitibrasil.cnpuv.embrapa.br/index.php\"\n",
    "re = requests.get(embrapa_url)\n",
    "soup = BeautifulSoup(re._content)\n",
    "# páginas\n",
    "pages = {}\n",
    "for i in soup.find_all('button'):\n",
    "    texto = i.text\n",
    "    value = '' if 'value' not in i.attrs.keys() else i.attrs['value']\n",
    "    pages[texto] = {\n",
    "        'link': f'{embrapa_url}?opcao={value}',\n",
    "        'has_subpages': 'No', \n",
    "        'sub_pages': {},\n",
    "        'download_link': {}\n",
    "        }\n",
    "    print(texto, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apresentação\n",
      "Produção\n",
      "Processamento\n",
      "subopt: subopt_01\n",
      "subopt: subopt_02\n",
      "subopt: subopt_03\n",
      "subopt: subopt_04\n",
      "Comercialização\n",
      "Importação\n",
      "subopt: subopt_01\n",
      "subopt: subopt_02\n",
      "subopt: subopt_03\n",
      "subopt: subopt_04\n",
      "subopt: subopt_05\n",
      "Exportação\n",
      "subopt: subopt_01\n",
      "subopt: subopt_02\n",
      "subopt: subopt_03\n",
      "subopt: subopt_04\n",
      "Publicação\n"
     ]
    }
   ],
   "source": [
    "# Sub Páginas\n",
    "\n",
    "for key, values in pages.items():\n",
    "    print(key)\n",
    "    re = requests.get(values['link'])\n",
    "    soup = BeautifulSoup(re._content)\n",
    "    subpages_dict = {}\n",
    "    for i in soup.find_all('button'):\n",
    "        text = i.text\n",
    "        btn_value = '' if 'value' not in i.attrs.keys() else i.attrs['value']\n",
    "        if 'subopt' in btn_value:\n",
    "            print(f\"subopt: {btn_value}\")\n",
    "            values['has_subpages'] = 'yes' \n",
    "            subpages_dict[text] = f\"{values['link']}&subopcao={btn_value}\"\n",
    "    values['sub_pages'] = subpages_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n",
      "Arquivo CSV baixado e salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Download Links\n",
    "for key, values in pages.items():\n",
    "    if key in ['Apresentação', 'Publicação']: continue\n",
    "    if values['has_subpages'] == 'No':\n",
    "        re = requests.get(values['link'])\n",
    "        soup = BeautifulSoup(re._content)\n",
    "        for link_element in soup.find_all('a', class_='footer_content'):\n",
    "            if 'href' in link_element.attrs.keys() and 'DOWNLOAD' in link_element.text:\n",
    "                values['download_link'] = [embrapa_url.replace('index.php', link_element['href'])]\n",
    "                # Fazendo a requisição para baixar o conteúdo do CSV\n",
    "                if 'pdf' in link_element['href']: continue\n",
    "                response = requests.get(embrapa_url.replace('index.php', link_element['href']))\n",
    "                # Checando se o download foi bem-sucedido\n",
    "                if response.status_code == 200:\n",
    "                    # Salvando o conteúdo do arquivo\n",
    "                    name = link_element['href'].split('/')[-1]\n",
    "                    with open(f\"data/data_{name}\", \"wb\") as file:\n",
    "                        file.write(response.content)\n",
    "                    print(\"Arquivo CSV baixado e salvo com sucesso.\")\n",
    "                else:\n",
    "                    print(f\"Erro ao baixar o arquivo: {response.status_code}\")\n",
    "    else:\n",
    "        download_link = []\n",
    "        for subpages in values['sub_pages'].values():\n",
    "            re = requests.get(subpages)\n",
    "            soup = BeautifulSoup(re._content)\n",
    "            for link_element in soup.find_all('a', class_='footer_content'):\n",
    "                if 'href' in link_element.attrs.keys() and 'DOWNLOAD' in link_element.text:\n",
    "                    download_link.append(embrapa_url.replace('index.php', link_element['href']))\n",
    "                    # Fazendo a requisição para baixar o conteúdo do CSV\n",
    "                    if 'pdf' in link_element['href']: continue\n",
    "                    response = requests.get(embrapa_url.replace('index.php', link_element['href']))\n",
    "                    # Checando se o download foi bem-sucedido\n",
    "                    if response.status_code == 200:\n",
    "                        # Salvando o conteúdo do arquivo\n",
    "                        name = link_element['href'].split('/')[-1]\n",
    "                        with open(f\"data/data_{name}\", \"wb\") as file:\n",
    "                            file.write(response.content)\n",
    "                        print(\"Arquivo CSV baixado e salvo com sucesso.\")\n",
    "                    else:\n",
    "                        print(f\"Erro ao baixar o arquivo: {response.status_code}\")\n",
    "        values['download_link'] = download_link\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
